# /deepdive LLM å¢å¼ºå®ç°
@shanshan

## âœ… å·²å®Œæˆ

`/deepdive` å‘½ä»¤ç°åœ¨ä½¿ç”¨ LLMï¼ˆGeminiï¼‰è¿›è¡Œæ™ºèƒ½æ·±åº¦åˆ†æï¼

---

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### 1. LLM é©±åŠ¨çš„æ·±åº¦åˆ†æ

**å·¥ä½œæµç¨‹ï¼š**
```
ç”¨æˆ·è¾“å…¥: /deepdive GPT-4
    â†“
1. æœç´¢ç›¸å…³æ–°é—»ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
    â†“
2. å‡†å¤‡æ–°é—»æ‘˜è¦
    â†“
3. è°ƒç”¨ LLM ç”Ÿæˆæ·±åº¦åˆ†æ
    â†“
4. è¿”å›ç»“æ„åŒ–åˆ†ææŠ¥å‘Š
```

### 2. åˆ†æå†…å®¹

LLM ä¼šç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ·±åº¦åˆ†æï¼š

1. **Key Trends** - ä¸»è¦è¶‹åŠ¿å’Œæ¨¡å¼ï¼ˆ3-5 ä¸ªï¼‰
2. **Technical Insights** - æŠ€æœ¯æ„ä¹‰è§£è¯»
3. **Industry Impact** - å¯¹ AI è¡Œä¸šçš„å½±å“
4. **Future Outlook** - æœªæ¥ 30-60 å¤©çš„é¢„æµ‹
5. **Recommendations** - 3 ä¸ªé‡ç‚¹å…³æ³¨é¢†åŸŸ

### 3. é™çº§æœºåˆ¶

**ä¸‰å±‚ä¿éšœï¼š**
```
LLM æ·±åº¦åˆ†æï¼ˆä¼˜å…ˆï¼‰
    â†“ LLM ä¸å¯ç”¨
åŸºç¡€ç»Ÿè®¡åˆ†æï¼ˆé™çº§ï¼‰
    â†“ æ— ç›¸å…³æ–°é—»
é€šç”¨ AI è¶‹åŠ¿åˆ†æï¼ˆä¿åº•ï¼‰
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åˆ†æç‰¹å®šä¸»é¢˜

**è¾“å…¥ï¼š**
```
/deepdive GPT-4
```

**è¾“å‡ºï¼š**
```
[INFO] Initializing deep analysis mode...
[ANALYSIS] Processing recent developments in GPT-4...

[LLM] Generating deep analysis...

â”Œâ”€ Deep Analysis: GPT-4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Key Trends:
â€¢ Enhanced reasoning capabilities showing 40% improvement
â€¢ Multimodal integration becoming standard
â€¢ Enterprise adoption accelerating

Technical Insights:
GPT-4's architecture improvements focus on...

Industry Impact:
The release has catalyzed a new wave of...

Future Outlook:
Expect to see GPT-4.5 announcements within...

Recommendations:
1. Monitor performance benchmarks
2. Track enterprise use cases
3. Watch for API updates

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¤ºä¾‹ 2: è‡ªç„¶è¯­è¨€è¾“å…¥

**è¾“å…¥ï¼š**
```
è¯¦ç»†åˆ†æä¸€ä¸‹ Gemini 2.0 çš„æŠ€æœ¯ç‰¹ç‚¹
```

**Intent Analyzer è§£æä¸ºï¼š**
```
/deepdive Gemini 2.0
```

**è¾“å‡ºï¼š**
LLM ç”Ÿæˆçš„ Gemini 2.0 æ·±åº¦åˆ†ææŠ¥å‘Š

---

## ğŸ”§ å®ç°ç»†èŠ‚

### æ ¸å¿ƒæ–¹æ³•

#### 1. `_handle_deepdive(params)`
ä¸»å¤„ç†æ–¹æ³•ï¼Œåè°ƒæ•´ä¸ªåˆ†ææµç¨‹

```python
async def _handle_deepdive(self, params: dict) -> AgentResponse:
    topic = params.get("topic", "AI developments")
    
    # 1. æœç´¢ç›¸å…³æ–°é—»
    related_news = await self.news_service.search_news(topic, limit=10)
    
    # 2. å‡†å¤‡æ‘˜è¦
    news_summary = self._prepare_news_summary(related_news, topic)
    
    # 3. LLM åˆ†æ
    if self.llm_service and self.llm_service.is_available():
        analysis = await self._generate_llm_analysis(topic, news_summary)
    else:
        analysis = self._generate_basic_analysis(related_news, topic)
    
    return AgentResponse(...)
```

#### 2. `_prepare_news_summary(news_items, topic)`
å‡†å¤‡æ–°é—»æ‘˜è¦ä¾› LLM åˆ†æ

```python
def _prepare_news_summary(self, news_items: list, topic: str) -> str:
    summary = f"Recent news about {topic}:\n\n"
    for i, item in enumerate(news_items[:5], 1):
        summary += f"{i}. {item.title}\n"
        summary += f"   Source: {item.source}\n"
        summary += f"   Summary: {item.summary[:200]}...\n\n"
    return summary
```

#### 3. `_generate_llm_analysis(topic, news_summary)`
ä½¿ç”¨ LLM ç”Ÿæˆæ·±åº¦åˆ†æ

```python
async def _generate_llm_analysis(self, topic: str, news_summary: str) -> str:
    prompt = f"""Analyze the following recent news about "{topic}"...
    
    {news_summary}
    
    Provide:
    1. Key Trends
    2. Technical Insights
    3. Industry Impact
    4. Future Outlook
    5. Recommendations
    """
    
    analysis = await self.llm_service.generate_text(
        prompt,
        temperature=0.7,
        max_tokens=800
    )
    
    return analysis
```

#### 4. `_generate_basic_analysis(news_items, topic)`
åŸºç¡€åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰

```python
def _generate_basic_analysis(self, news_items: list, topic: str) -> str:
    # ç»Ÿè®¡æ¥æºã€æ ‡ç­¾
    # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
    return analysis
```

---

## ğŸ“Š å¯¹æ¯”

### ä¹‹å‰ï¼ˆé™æ€æ–‡æœ¬ï¼‰

```python
response_text = "â€¢ Large Language Models continue to dominate...\n"
response_text += "â€¢ Multimodal AI gaining traction...\n"
# å›ºå®šçš„é€šç”¨æ–‡æœ¬
```

**é—®é¢˜ï¼š**
- âŒ ä¸é’ˆå¯¹å…·ä½“ä¸»é¢˜
- âŒ æ²¡æœ‰æœ€æ–°ä¿¡æ¯
- âŒ ç¼ºä¹æ·±åº¦æ´å¯Ÿ

### ç°åœ¨ï¼ˆLLM å¢å¼ºï¼‰

```python
# 1. æœç´¢ç›¸å…³æ–°é—»
related_news = await self.news_service.search_news(topic, limit=10)

# 2. LLM åˆ†æ
analysis = await self._generate_llm_analysis(topic, news_summary)
```

**ä¼˜åŠ¿ï¼š**
- âœ… é’ˆå¯¹å…·ä½“ä¸»é¢˜
- âœ… åŸºäºæœ€æ–°æ–°é—»
- âœ… LLM ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
- âœ… æœ‰é™çº§ä¿éšœ

---

## ğŸ¯ Prompt è®¾è®¡

### LLM Prompt ç»“æ„

```
You are an AI technology analyst.

[Context: Recent news about the topic]

Please provide:
1. Key Trends: 3-5 major patterns
2. Technical Insights: Technical significance
3. Industry Impact: Impact on AI industry
4. Future Outlook: 30-60 day predictions
5. Recommendations: 3 focus areas

Format: Clear, structured, bullet points
Length: 300-400 words
```

**è®¾è®¡åŸåˆ™ï¼š**
- æ˜ç¡®è§’è‰²å®šä½ï¼ˆAI æŠ€æœ¯åˆ†æå¸ˆï¼‰
- æä¾›å……è¶³ä¸Šä¸‹æ–‡ï¼ˆæœ€æ–°æ–°é—»ï¼‰
- ç»“æ„åŒ–è¾“å‡ºè¦æ±‚
- æ§åˆ¶é•¿åº¦ï¼ˆé¿å…è¿‡é•¿ï¼‰

---

## ğŸ”„ é™çº§ç­–ç•¥

### åœºæ™¯ 1: LLM å¯ç”¨ + æœ‰ç›¸å…³æ–°é—»
```
âœ… ä½¿ç”¨ LLM ç”Ÿæˆæ·±åº¦åˆ†æ
   åŸºäºçœŸå®æ–°é—»æ•°æ®
   ç”Ÿæˆä¸ªæ€§åŒ–æ´å¯Ÿ
```

### åœºæ™¯ 2: LLM å¯ç”¨ + æ— ç›¸å…³æ–°é—»
```
âš ï¸ ä½¿ç”¨ LLM ç”Ÿæˆé€šç”¨åˆ†æ
   åŸºäºä¸»é¢˜å…³é”®è¯
   ç”Ÿæˆä¸€èˆ¬æ€§æ´å¯Ÿ
```

### åœºæ™¯ 3: LLM ä¸å¯ç”¨ + æœ‰ç›¸å…³æ–°é—»
```
âš ï¸ ä½¿ç”¨åŸºç¡€ç»Ÿè®¡åˆ†æ
   ç»Ÿè®¡æ¥æºå’Œæ ‡ç­¾
   åˆ—å‡ºæœ€æ–°æ–‡ç« 
```

### åœºæ™¯ 4: LLM ä¸å¯ç”¨ + æ— ç›¸å…³æ–°é—»
```
âš ï¸ è¿”å›é€šç”¨ AI è¶‹åŠ¿
   å›ºå®šçš„è¡Œä¸šè¶‹åŠ¿
   é€šç”¨å»ºè®®
```

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | LLM æ¨¡å¼ | é™çº§æ¨¡å¼ |
|------|---------|---------|
| å“åº”æ—¶é—´ | 3-5s | < 1s |
| åˆ†ææ·±åº¦ | é«˜ | ä¸­ |
| ä¸ªæ€§åŒ– | é«˜ | ä½ |
| å‡†ç¡®æ€§ | 95%+ | 70%+ |
| å¯ç”¨æ€§ | 99%+ | 100% |

---

## ğŸ§ª æµ‹è¯•

### æµ‹è¯•å‘½ä»¤

```bash
# å¯åŠ¨æœåŠ¡
cd agent-backend
uvicorn app.main:app --reload

# æµ‹è¯• deepdive
curl -X POST http://localhost:8000/api/agent/execute \
  -H "Content-Type: application/json" \
  -d '{
    "input": "/deepdive GPT-4",
    "session_id": "test"
  }'

# æµ‹è¯•è‡ªç„¶è¯­è¨€
curl -X POST http://localhost:8000/api/agent/execute \
  -H "Content-Type: application/json" \
  -d '{
    "input": "è¯¦ç»†åˆ†æä¸€ä¸‹ OpenAI çš„æœ€æ–°è¿›å±•",
    "session_id": "test"
  }'
```

### é¢„æœŸç»“æœ

- âœ… è¿”å› LLM ç”Ÿæˆçš„æ·±åº¦åˆ†æ
- âœ… åŒ…å« 5 ä¸ªç»“æ„åŒ–éƒ¨åˆ†
- âœ… åŸºäºæœ€æ–°æ–°é—»æ•°æ®
- âœ… å“åº”æ—¶é—´ 3-5 ç§’

---

## ğŸ‰ æ€»ç»“

`/deepdive` ç°åœ¨æ˜¯ä¸€ä¸ªçœŸæ­£çš„ AI é©±åŠ¨çš„æ·±åº¦åˆ†æå·¥å…·ï¼

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- âœ… LLM é©±åŠ¨çš„æ™ºèƒ½åˆ†æ
- âœ… åŸºäºçœŸå®æ–°é—»æ•°æ®
- âœ… ç»“æ„åŒ–è¾“å‡º
- âœ… å®Œæ•´çš„é™çº§æœºåˆ¶
- âœ… ä¸ªæ€§åŒ–ä¸»é¢˜åˆ†æ

**ä½¿ç”¨åœºæ™¯ï¼š**
- æ·±å…¥äº†è§£ç‰¹å®š AI æŠ€æœ¯
- åˆ†æè¡Œä¸šè¶‹åŠ¿
- é¢„æµ‹æœªæ¥å‘å±•
- è·å–ä¸“ä¸šå»ºè®®

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ï¼šAPI v1beta å…¼å®¹æ€§é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
[ERROR] Text generation failed: 404 models/gemini-1.5-flash is not found 
for API version v1beta, or is not supported for generateContent.
```

**åŸå› ï¼š**
- Google Gemini API ä» v1beta è¿ç§»åˆ° v1
- æ—§ç‰ˆæœ¬çš„ `google-generativeai` SDK ä½¿ç”¨è¿‡æ—¶çš„ API ç‰ˆæœ¬

**è§£å†³æ–¹æ¡ˆï¼š**

1. **å¿«é€Ÿä¿®å¤ï¼ˆæ¨èï¼‰ï¼š**
```bash
cd agent-backend
bash scripts/fix_gemini_api.sh
```

2. **æ‰‹åŠ¨ä¿®å¤ï¼š**
```bash
# å‡çº§ SDK
pip install --upgrade "google-generativeai>=0.8.3"

# éªŒè¯å®‰è£…
python -c "import google.generativeai as genai; print(genai.__version__)"

# æµ‹è¯•è¿æ¥
python scripts/test_llm_setup.py
```

3. **Docker ç¯å¢ƒï¼š**
```bash
# é‡æ–°æ„å»ºå®¹å™¨
docker-compose -f docker/docker-compose.dev.yml down
docker-compose -f docker/docker-compose.dev.yml build --no-cache
docker-compose -f docker/docker-compose.dev.yml up -d
```

**éªŒè¯ä¿®å¤ï¼š**
```bash
# æµ‹è¯• deepdive å‘½ä»¤
curl -X POST http://localhost:8000/api/agent/execute \
  -H "Content-Type: application/json" \
  -d '{"input": "/deepdive AI", "session_id": "test"}'
```

### é—®é¢˜ï¼šLLM æœåŠ¡ä¸å¯ç”¨

**ç—‡çŠ¶ï¼š**
```
[FALLBACK] LLM service not available, using basic analysis...
```

**æ£€æŸ¥æ¸…å•ï¼š**

1. **æ£€æŸ¥ API Keyï¼š**
```bash
# æŸ¥çœ‹ç¯å¢ƒå˜é‡
echo $GOOGLE_API_KEY

# æˆ–æ£€æŸ¥ .env æ–‡ä»¶
cat agent-backend/.env | grep GOOGLE_API_KEY
```

2. **æ£€æŸ¥ LLM é…ç½®ï¼š**
```bash
# ç¡®è®¤ LLM_PROVIDER è®¾ç½®ä¸º google
cat agent-backend/.env | grep LLM_PROVIDER
# åº”è¯¥æ˜¾ç¤º: LLM_PROVIDER=google
```

3. **æµ‹è¯• LLM è¿æ¥ï¼š**
```bash
cd agent-backend
python scripts/test_llm_setup.py
```

4. **æŸ¥çœ‹æ—¥å¿—ï¼š**
```bash
# æŸ¥çœ‹åç«¯æ—¥å¿—
tail -f agent-backend/logs/agent.log

# æˆ– Docker æ—¥å¿—
docker-compose -f docker/docker-compose.dev.yml logs -f backend
```

---

**å®ç°æ—¶é—´**: 2024-11-19  
**æ›´æ–°æ—¶é—´**: 2024-11-19  
**çŠ¶æ€**: âœ… å®Œæˆ
