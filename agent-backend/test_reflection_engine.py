#!/usr/bin/env python3
"""
æµ‹è¯• ReflectionEngine å®ç°

éªŒè¯ Phase 5 çš„æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.core.reflection_engine import ReflectionEngine, get_reflection_engine
from app.models.react import ReActStep, ExecutionPlan, PlanStep, QualityEvaluation
from app.models.tool import ToolCall, ToolResult
from datetime import datetime


def test_reflection_engine_initialization():
    """æµ‹è¯• ReflectionEngine åˆå§‹åŒ–"""
    print("ğŸ§ª Testing ReflectionEngine initialization...")
    
    engine = get_reflection_engine()
    assert engine is not None, "ReflectionEngine should be initialized"
    assert engine.llm_service is not None, "LLM service should be available"
    
    print("âœ… ReflectionEngine initialized successfully")


def test_should_continue():
    """æµ‹è¯• should_continue æ–¹æ³•"""
    print("\nğŸ§ª Testing should_continue() method...")
    
    engine = get_reflection_engine()
    
    # åˆ›å»ºæµ‹è¯•è®¡åˆ’
    plan = ExecutionPlan(
        query='test query',
        complexity='simple',
        steps=[PlanStep(
            step_number=1,
            description='test',
            tool_name='test_tool',
            parameters={},
            required=True
        )],
        estimated_iterations=2
    )
    
    # åˆ›å»ºæˆåŠŸçš„æ­¥éª¤
    step = ReActStep(
        step_number=1,
        thought='test thought',
        action=ToolCall(
            tool_name='test_tool',
            parameters={},
            reasoning='test',
            confidence=0.8,
            source='test'
        ),
        observation=ToolResult(
            success=True,
            data='test result',
            execution_time=0.1,
            tool_name='test_tool'
        ),
        status='completed',
        timestamp=datetime.now()
    )
    
    # æµ‹è¯• 1: ç¬¬ä¸€æ­¥ååº”è¯¥ç»§ç»­
    should_continue = engine.should_continue([step], plan)
    assert should_continue == True, "Should continue after first successful step"
    print("âœ… Test 1 passed: Continues after first step")
    
    # æµ‹è¯• 2: è¾¾åˆ°ä¼°è®¡è¿­ä»£æ¬¡æ•°ååº”è¯¥åœæ­¢
    step2 = ReActStep(
        step_number=2,
        thought='test thought 2',
        action=ToolCall(
            tool_name='test_tool',
            parameters={},
            reasoning='test',
            confidence=0.8,
            source='test'
        ),
        observation=ToolResult(
            success=True,
            data='test result 2',
            execution_time=0.1,
            tool_name='test_tool'
        ),
        status='completed',
        timestamp=datetime.now()
    )
    
    should_continue = engine.should_continue([step, step2], plan)
    assert should_continue == False, "Should stop after reaching estimated iterations"
    print("âœ… Test 2 passed: Stops after reaching estimated iterations")
    
    # æµ‹è¯• 3: å¤±è´¥æ­¥éª¤ååº”è¯¥åœæ­¢
    failed_step = ReActStep(
        step_number=1,
        thought='test thought',
        action=ToolCall(
            tool_name='test_tool',
            parameters={},
            reasoning='test',
            confidence=0.8,
            source='test'
        ),
        observation=ToolResult(
            success=False,
            error='test error',
            execution_time=0.1,
            tool_name='test_tool'
        ),
        status='failed',
        timestamp=datetime.now()
    )
    
    should_continue = engine.should_continue([failed_step], plan)
    assert should_continue == False, "Should stop after failed step"
    print("âœ… Test 3 passed: Stops after failed step")
    
    # æµ‹è¯• 4: è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°åº”è¯¥åœæ­¢
    many_steps = [step] * 5  # 5 ä¸ªæ­¥éª¤
    should_continue = engine.should_continue(many_steps, plan)
    assert should_continue == False, "Should stop when exceeding max iterations"
    print("âœ… Test 4 passed: Stops when exceeding max iterations")


def test_clamp_score():
    """æµ‹è¯•è¯„åˆ†é™åˆ¶æ–¹æ³•"""
    print("\nğŸ§ª Testing _clamp_score() method...")
    
    engine = get_reflection_engine()
    
    # æµ‹è¯•è¶…å‡ºä¸Šé™
    score = engine._clamp_score(15)
    assert score == 10, f"Score should be clamped to 10, got {score}"
    print("âœ… Test 1 passed: Clamps high scores to 10")
    
    # æµ‹è¯•è¶…å‡ºä¸‹é™
    score = engine._clamp_score(-5)
    assert score == 0, f"Score should be clamped to 0, got {score}"
    print("âœ… Test 2 passed: Clamps low scores to 0")
    
    # æµ‹è¯•æ­£å¸¸èŒƒå›´
    score = engine._clamp_score(7)
    assert score == 7, f"Score should remain 7, got {score}"
    print("âœ… Test 3 passed: Keeps valid scores unchanged")
    
    # æµ‹è¯•æ— æ•ˆè¾“å…¥
    score = engine._clamp_score("invalid")
    assert score == 5, f"Invalid input should default to 5, got {score}"
    print("âœ… Test 4 passed: Handles invalid input gracefully")


def test_fallback_evaluation():
    """æµ‹è¯•é™çº§è¯„ä¼°æ–¹æ³•"""
    print("\nğŸ§ª Testing _fallback_evaluation() method...")
    
    engine = get_reflection_engine()
    
    # æµ‹è¯•ç©ºæ­¥éª¤åˆ—è¡¨
    evaluation = engine._fallback_evaluation("", [])
    assert evaluation.completeness_score == 0, "Empty steps should score 0"
    assert evaluation.needs_retry == True, "Empty steps should need retry"
    print("âœ… Test 1 passed: Handles empty steps correctly")
    
    # æµ‹è¯•æˆåŠŸæ­¥éª¤
    step = ReActStep(
        step_number=1,
        thought='test thought',
        action=ToolCall(
            tool_name='test_tool',
            parameters={},
            reasoning='test',
            confidence=0.8,
            source='test'
        ),
        observation=ToolResult(
            success=True,
            data='test result',
            execution_time=0.1,
            tool_name='test_tool'
        ),
        status='completed',
        timestamp=datetime.now()
    )
    
    output = "This is a test output with sufficient length to be considered quality content."
    evaluation = engine._fallback_evaluation(output, [step])
    assert evaluation.completeness_score == 10, "All successful steps should score 10"
    assert evaluation.quality_score >= 8, "Good output should have high quality score"
    assert evaluation.needs_retry == False, "High score should not need retry"
    print("âœ… Test 2 passed: Evaluates successful steps correctly")
    
    # æµ‹è¯•éƒ¨åˆ†å¤±è´¥
    failed_step = ReActStep(
        step_number=2,
        thought='test thought',
        action=ToolCall(
            tool_name='test_tool',
            parameters={},
            reasoning='test',
            confidence=0.8,
            source='test'
        ),
        observation=ToolResult(
            success=False,
            error='test error',
            execution_time=0.1,
            tool_name='test_tool'
        ),
        status='failed',
        timestamp=datetime.now()
    )
    
    evaluation = engine._fallback_evaluation(output, [step, failed_step])
    assert evaluation.completeness_score == 5, "50% success rate should score 5"
    assert len(evaluation.missing_info) > 0, "Failed steps should be in missing_info"
    assert evaluation.needs_retry == True, "Low score should need retry"
    print("âœ… Test 3 passed: Handles partial failures correctly")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("ReflectionEngine Test Suite")
    print("=" * 60)
    
    try:
        test_reflection_engine_initialization()
        test_should_continue()
        test_clamp_score()
        test_fallback_evaluation()
        
        print("\n" + "=" * 60)
        print("âœ… All tests passed!")
        print("=" * 60)
        return 0
    
    except AssertionError as e:
        print(f"\nâŒ Test failed: {e}")
        return 1
    
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
