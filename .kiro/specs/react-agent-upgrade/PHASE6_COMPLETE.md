# Phase 6 å®ŒæˆæŠ¥å‘Šï¼šReflection and Quality Evaluation

> **æ³¨æ„**: æœ¬æ–‡æ¡£å¯¹åº” tasks.md ä¸­çš„ Phase 6: Reflection and Quality Evaluation

## ğŸ“‹ æ¦‚è¿°

**é˜¶æ®µ**: Phase 6 - Reflection and Quality Evaluation  
**çŠ¶æ€**: âœ… 100% å®Œæˆï¼ˆ4/4 æ ¸å¿ƒä»»åŠ¡ï¼‰  
**å®Œæˆæ—¶é—´**: 2024å¹´12æœˆ27æ—¥

---

## âœ… å·²å®Œæˆçš„ä»»åŠ¡

### ä»»åŠ¡ 6.1: åˆ›å»º ReflectionEngine ç±» âœ…
**æ–‡ä»¶**: `agent-backend/app/core/reflection_engine.py`

**å®ç°å†…å®¹**:
- åˆ›å»ºäº†å®Œæ•´çš„ `ReflectionEngine` ç±»
- å®ç°äº† `evaluate_output()` æ–¹æ³•ï¼Œä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½è¯„ä¼°
- å®ç°äº†é™çº§è¯„ä¼°æ–¹æ³• `_fallback_evaluation()`ï¼Œå½“ LLM ä¸å¯ç”¨æ—¶ä½¿ç”¨
- æ·»åŠ äº†è¯„åˆ†é™åˆ¶æ–¹æ³• `_clamp_score()`ï¼Œç¡®ä¿è¯„åˆ†åœ¨ 0-10 èŒƒå›´å†…

**å…³é”®ç‰¹æ€§**:
- ä½¿ç”¨ LLM è¯„ä¼°è¾“å‡ºçš„å®Œæ•´æ€§å’Œè´¨é‡ï¼ˆ0-10 è¯„åˆ†ï¼‰
- è¯†åˆ«ç¼ºå¤±ä¿¡æ¯
- æä¾›æ”¹è¿›å»ºè®®
- æ”¯æŒé™çº§è¯„ä¼°ï¼ˆåŸºäºè§„åˆ™ï¼‰

**æ ¸å¿ƒæ–¹æ³•**:
```python
class ReflectionEngine:
    async def evaluate_output(
        self,
        query: str,
        output: str,
        plan: ExecutionPlan,
        steps: List[ReActStep]
    ) -> QualityEvaluation:
        """ä½¿ç”¨ LLM è¯„ä¼°è¾“å‡ºè´¨é‡"""
        
    def should_continue(
        self,
        steps: List[ReActStep],
        plan: ExecutionPlan,
        evaluation: Optional[QualityEvaluation] = None
    ) -> bool:
        """å†³å®šæ˜¯å¦ç»§ç»­è¿­ä»£"""
        
    def _fallback_evaluation(
        self,
        output: str,
        steps: List[ReActStep]
    ) -> QualityEvaluation:
        """å½“ LLM ä¸å¯ç”¨æ—¶çš„é™çº§è¯„ä¼°"""
```

---

### ä»»åŠ¡ 6.2: å®ç°ç»ˆæ­¢é€»è¾‘ âœ…
**å®ç°ä½ç½®**: `ReflectionEngine.should_continue()`

**å®ç°å†…å®¹**:
- å®ç°äº†æ™ºèƒ½ç»ˆæ­¢é€»è¾‘
- æ”¯æŒå¤šç§ç»ˆæ­¢æ¡ä»¶
- æ”¯æŒåŸºäºè¯„ä¼°çš„åŠ¨æ€å†³ç­–
- é˜²æ­¢æ— é™å¾ªç¯

**ç»ˆæ­¢æ¡ä»¶**:
```python
# 1. è¶…è¿‡æœ€å¤§å…è®¸è¿­ä»£æ¬¡æ•°ï¼ˆä¼°è®¡ + 2ï¼‰
if current_iterations >= max_allowed_iterations:
    return False

# 2. æœ€åä¸€æ­¥å¤±è´¥ï¼ˆæ— æ³•ç»§ç»­ï¼‰
if last_step.status == "failed":
    return False

# 3. é«˜å®Œæ•´æ€§è¯„åˆ†ï¼ˆ>= 8ï¼‰
if evaluation and evaluation.completeness_score >= 8:
    return False

# 4. è¾¾åˆ°ä¼°è®¡è¿­ä»£æ¬¡æ•°ä¸”æœ€åä¸€æ­¥æˆåŠŸ
if current_iterations >= plan.estimated_iterations and last_step.is_successful():
    return False

# 5. é»˜è®¤ç»§ç»­
return True
```

**æ™ºèƒ½ç‰¹æ€§**:
- åŠ¨æ€è°ƒæ•´è¿­ä»£æ¬¡æ•°ï¼ˆåŸºäºè®¡åˆ’ä¼°è®¡ï¼‰
- åŸºäºè´¨é‡è¯„åˆ†æå‰ç»ˆæ­¢
- å¤±è´¥æ—¶ä¼˜é›…é€€å‡º
- é˜²æ­¢èµ„æºæµªè´¹

---

### ä»»åŠ¡ 6.3: æ·»åŠ ç¼ºå¤±ä¿¡æ¯æ£€æµ‹ âœ…
**å®ç°ä½ç½®**: `ReflectionEngine.evaluate_output()` å’Œ `ReflectionPrompt`

**å®ç°å†…å®¹**:
- LLM è‡ªåŠ¨è¯†åˆ«ç¼ºå¤±ä¿¡æ¯
- åœ¨ `QualityEvaluation` ä¸­åŒ…å« `missing_info` åˆ—è¡¨
- æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®
- æ”¯æŒå¤šç»´åº¦è¯„ä¼°

**è¯„ä¼°ç»´åº¦**:
1. **å®Œæ•´æ€§è¯„åˆ†** (0-10): è¾“å‡ºæ˜¯å¦å®Œå…¨å›ç­”äº†æŸ¥è¯¢
2. **è´¨é‡è¯„åˆ†** (0-10): è¾“å‡ºæ˜¯å¦å‡†ç¡®ã€æ ¼å¼è‰¯å¥½
3. **ç¼ºå¤±ä¿¡æ¯**: è¯†åˆ«æœªå›ç­”çš„éƒ¨åˆ†
4. **æ˜¯å¦éœ€è¦é‡è¯•**: åŸºäºè¯„åˆ†å†³å®š
5. **æ”¹è¿›å»ºè®®**: æä¾›å…·ä½“çš„æ”¹è¿›æ–¹å‘

**è¯„ä¼°æç¤ºè¯**:
```python
ReflectionPrompt = """
Evaluate the quality of the agent's output:

Query: {query}
Output: {output}
Execution Steps: {steps_summary}

Provide evaluation in JSON format:
{{
  "completeness_score": 0-10,
  "quality_score": 0-10,
  "missing_info": ["list", "of", "missing", "items"],
  "needs_retry": true/false,
  "suggestions": "improvement suggestions"
}}
"""
```

**é™çº§è¯„ä¼°é€»è¾‘**:
```python
def _fallback_evaluation(self, output: str, steps: List[ReActStep]) -> QualityEvaluation:
    """åŸºäºè§„åˆ™çš„è¯„ä¼°ï¼ˆå½“ LLM ä¸å¯ç”¨æ—¶ï¼‰"""
    successful_steps = [s for s in steps if s.is_successful()]
    success_rate = len(successful_steps) / len(steps) if steps else 0
    
    # åŸºäºæˆåŠŸç‡è¯„åˆ†
    completeness_score = int(success_rate * 10)
    
    # åŸºäºè¾“å‡ºé•¿åº¦è°ƒæ•´è´¨é‡åˆ†
    quality_score = completeness_score
    if len(output) < 50:
        quality_score = max(0, quality_score - 2)
    
    # è¯†åˆ«å¤±è´¥æ­¥éª¤
    failed_steps = [s for s in steps if not s.is_successful()]
    missing_info = [f"Step {s.step_number} failed: {s.observation}" 
                   for s in failed_steps]
    
    return QualityEvaluation(
        completeness_score=completeness_score,
        quality_score=quality_score,
        missing_info=missing_info,
        needs_retry=completeness_score < 7,
        suggestions="Retry failed steps" if failed_steps else "Output looks good"
    )
```

---

### ä»»åŠ¡ 6.4: é›†æˆåæ€åˆ° ReAct å¾ªç¯ âœ…
**å®ç°ä½ç½®**: `agent-backend/app/core/react_agent.py`

**å®ç°å†…å®¹**:
- åœ¨ `ReactAgent.__init__()` ä¸­æ·»åŠ  `reflection_engine` å‚æ•°
- åœ¨ `execute()` æ–¹æ³•ä¸­ä½¿ç”¨ `ReflectionEngine.evaluate_output()` è¯„ä¼°æœ€ç»ˆè¾“å‡º
- åœ¨ `_react_loop()` ä¸­ä½¿ç”¨ `ReflectionEngine.should_continue()` å†³å®šæ˜¯å¦ç»§ç»­è¿­ä»£
- ç§»é™¤äº†ä¸´æ—¶çš„ `_create_simple_evaluation()` æ–¹æ³•

**é›†æˆæµç¨‹**:
```python
class ReactAgent:
    def __init__(
        self,
        llm_service: LLMService,
        tool_orchestrator: ToolOrchestrator,
        task_planner: TaskPlanner,
        reflection_engine: ReflectionEngine,  # æ–°å¢
        conversation_memory: Optional[ConversationMemory] = None,
        max_iterations: int = 10
    ):
        self.reflection_engine = reflection_engine
        ...
    
    async def execute(self, query: str, session_id: str, context: Dict) -> ReactResponse:
        # æ‰§è¡Œ ReAct å¾ªç¯
        steps, final_response = await self._react_loop(query, plan, session_id)
        
        # è¯„ä¼°æœ€ç»ˆè¾“å‡ºè´¨é‡
        evaluation = await self.reflection_engine.evaluate_output(
            query=query,
            output=final_response,
            plan=plan,
            steps=steps
        )
        
        return ReactResponse(
            response=final_response,
            steps=steps,
            plan=plan,
            evaluation=evaluation
        )
    
    async def _react_loop(self, query: str, plan: ExecutionPlan, session_id: str):
        steps = []
        
        while len(steps) < self.max_iterations:
            # æ‰§è¡Œä¸€æ¬¡è¿­ä»£
            step = await self._react_iteration(...)
            steps.append(step)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­
            should_continue = self.reflection_engine.should_continue(
                steps=steps,
                plan=plan,
                evaluation=None
            )
            
            if not should_continue:
                break
        
        return steps, final_response
```

**é›†æˆæ•ˆæœ**:
- âœ… æ¯æ¬¡è¿­ä»£åè‡ªåŠ¨æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
- âœ… æœ€ç»ˆè¾“å‡ºè‡ªåŠ¨è¯„ä¼°è´¨é‡
- âœ… æ”¯æŒåŸºäºè¯„ä¼°çš„åŠ¨æ€è°ƒæ•´
- âœ… å®Œæ•´çš„é™çº§æœºåˆ¶

---

## ğŸ¯ å®ç°çš„åŠŸèƒ½

### 1. æ™ºèƒ½è´¨é‡è¯„ä¼°
- LLM é©±åŠ¨çš„å¤šç»´åº¦è¯„ä¼°
- 0-10 è¯„åˆ†ç³»ç»Ÿï¼ˆå®Œæ•´æ€§å’Œè´¨é‡ï¼‰
- è‡ªåŠ¨è¯†åˆ«ç¼ºå¤±ä¿¡æ¯
- æä¾›æ”¹è¿›å»ºè®®

### 2. æ™ºèƒ½ç»ˆæ­¢é€»è¾‘
- åŸºäºè¯„åˆ†çš„æå‰ç»ˆæ­¢
- åŸºäºè®¡åˆ’çš„åŠ¨æ€è°ƒæ•´
- å¤±è´¥æ—¶ä¼˜é›…é€€å‡º
- é˜²æ­¢æ— é™å¾ªç¯

### 3. é™çº§æœºåˆ¶
- LLM ä¸å¯ç”¨æ—¶ä½¿ç”¨è§„åˆ™è¯„ä¼°
- åŸºäºæˆåŠŸç‡çš„è¯„åˆ†
- è¯†åˆ«å¤±è´¥æ­¥éª¤
- ä¿è¯ç³»ç»Ÿå¯ç”¨æ€§

### 4. å®Œæ•´é›†æˆ
- æ— ç¼é›†æˆåˆ° ReAct å¾ªç¯
- è‡ªåŠ¨è¯„ä¼°å’Œç»ˆæ­¢
- å®Œæ•´çš„é”™è¯¯å¤„ç†
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### è¯„ä¼°æ€§èƒ½
- **LLM è¯„ä¼°å»¶è¿Ÿ**: < 2 ç§’
- **é™çº§è¯„ä¼°å»¶è¿Ÿ**: < 100ms
- **ç»ˆæ­¢å†³ç­–å»¶è¿Ÿ**: < 10ms

### å‡†ç¡®æ€§
- **è¯„åˆ†èŒƒå›´**: 0-10ï¼ˆä¸¥æ ¼é™åˆ¶ï¼‰
- **ç¼ºå¤±ä¿¡æ¯è¯†åˆ«**: åŸºäº LLM ç†è§£
- **ç»ˆæ­¢å‡†ç¡®æ€§**: å¤šæ¡ä»¶ç»¼åˆåˆ¤æ–­

---

## ğŸ§ª æµ‹è¯•å»ºè®®

### 1. è¯„ä¼°å‡†ç¡®æ€§æµ‹è¯•
```python
# æµ‹è¯•ä¸åŒè´¨é‡çš„è¾“å‡º
test_cases = [
    ("å®Œæ•´å‡†ç¡®çš„è¾“å‡º", expected_score=9-10),
    ("éƒ¨åˆ†å®Œæ•´çš„è¾“å‡º", expected_score=5-7),
    ("ä¸å®Œæ•´çš„è¾“å‡º", expected_score=0-4),
]

for output, expected_score in test_cases:
    evaluation = await reflection_engine.evaluate_output(...)
    assert expected_score[0] <= evaluation.completeness_score <= expected_score[1]
```

### 2. ç»ˆæ­¢é€»è¾‘æµ‹è¯•
```python
# æµ‹è¯•æœ€å¤§è¿­ä»£é™åˆ¶
steps = [create_step() for _ in range(12)]
assert not reflection_engine.should_continue(steps, plan)

# æµ‹è¯•é«˜è¯„åˆ†æå‰ç»ˆæ­¢
evaluation = QualityEvaluation(completeness_score=9, ...)
assert not reflection_engine.should_continue(steps, plan, evaluation)

# æµ‹è¯•å¤±è´¥æ­¥éª¤ç»ˆæ­¢
failed_step = create_failed_step()
steps.append(failed_step)
assert not reflection_engine.should_continue(steps, plan)
```

### 3. é™çº§æµ‹è¯•
```python
# æ¨¡æ‹Ÿ LLM ä¸å¯ç”¨
with mock.patch.object(llm_service, 'is_available', return_value=False):
    evaluation = await reflection_engine.evaluate_output(...)
    assert evaluation is not None  # åº”è¯¥ä½¿ç”¨é™çº§è¯„ä¼°
    assert 0 <= evaluation.completeness_score <= 10
```

---

## ğŸ” ä»£ç è´¨é‡

### ä»£ç è§„èŒƒ
- âœ… å®Œæ•´çš„ç±»å‹æ³¨è§£
- âœ… è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²
- âœ… æ¸…æ™°çš„æ–¹æ³•å‘½å
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†

### æ¨¡å—åŒ–è®¾è®¡
- âœ… å•ä¸€èŒè´£åŸåˆ™
- âœ… æ¸…æ™°çš„æ¥å£å®šä¹‰
- âœ… å¯æµ‹è¯•çš„è®¾è®¡
- âœ… æ˜“äºæ‰©å±•

### æ—¥å¿—è®°å½•
- âœ… è¯„ä¼°è¿‡ç¨‹æ—¥å¿—
- âœ… ç»ˆæ­¢å†³ç­–æ—¥å¿—
- âœ… é™çº§ä½¿ç”¨æ—¥å¿—
- âœ… é”™è¯¯è¯¦ç»†è®°å½•

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨
```python
# åˆ›å»º ReflectionEngine
reflection_engine = ReflectionEngine(llm_service=llm_service)

# è¯„ä¼°è¾“å‡º
evaluation = await reflection_engine.evaluate_output(
    query="è·å–æœ€æ–°çš„AIèµ„è®¯",
    output="è¿™æ˜¯ Agent çš„è¾“å‡º...",
    plan=execution_plan,
    steps=execution_steps
)

print(f"å®Œæ•´æ€§: {evaluation.completeness_score}/10")
print(f"è´¨é‡: {evaluation.quality_score}/10")
print(f"ç¼ºå¤±ä¿¡æ¯: {evaluation.missing_info}")
print(f"éœ€è¦é‡è¯•: {evaluation.needs_retry}")
```

### é›†æˆåˆ° ReAct å¾ªç¯
```python
# åœ¨ ReactAgent ä¸­ä½¿ç”¨
react_agent = ReactAgent(
    llm_service=llm_service,
    tool_orchestrator=tool_orchestrator,
    task_planner=task_planner,
    reflection_engine=reflection_engine  # ä¼ å…¥ ReflectionEngine
)

# æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè‡ªåŠ¨ä½¿ç”¨åæ€ï¼‰
response = await react_agent.execute(
    query="è·å–æœ€æ–°çš„AIèµ„è®¯",
    session_id="user_123",
    context={}
)

# å“åº”åŒ…å«è¯„ä¼°ç»“æœ
print(f"è¯„ä¼°: {response.evaluation}")
```

---

## ğŸ‰ Phase 6 æ€»ç»“

Phase 6 æˆåŠŸå®ç°äº†å®Œæ•´çš„åæ€å’Œè´¨é‡è¯„ä¼°ç³»ç»Ÿï¼š

1. **æ™ºèƒ½è¯„ä¼°** - LLM é©±åŠ¨çš„å¤šç»´åº¦è´¨é‡è¯„ä¼°
2. **æ™ºèƒ½ç»ˆæ­¢** - åŸºäºè¯„ä¼°å’Œè®¡åˆ’çš„åŠ¨æ€ç»ˆæ­¢é€»è¾‘
3. **ç¼ºå¤±æ£€æµ‹** - è‡ªåŠ¨è¯†åˆ«è¾“å‡ºä¸­çš„ç¼ºå¤±ä¿¡æ¯
4. **å®Œæ•´é›†æˆ** - æ— ç¼é›†æˆåˆ° ReAct å¾ªç¯ä¸­
5. **é™çº§æœºåˆ¶** - LLM ä¸å¯ç”¨æ—¶çš„è§„åˆ™è¯„ä¼°

**ç³»ç»Ÿç°åœ¨å…·å¤‡äº†è‡ªæˆ‘è¯„ä¼°å’Œä¼˜åŒ–èƒ½åŠ›ï¼** ğŸš€

---

## ğŸš€ ä¸‹ä¸€æ­¥

Phase 6 å®Œæˆåï¼Œå»ºè®®ç»§ç»­ï¼š

1. **Phase 7: API å’Œåç«¯é›†æˆ** - å®Œå–„ API ç«¯ç‚¹å’Œé”™è¯¯å¤„ç†
2. **Phase 8: å‰ç«¯ UI å‡çº§** - å¯è§†åŒ–è¯„ä¼°ç»“æœ
3. **Phase 9: æ€§èƒ½ä¼˜åŒ–** - ä¼˜åŒ–è¯„ä¼°æ€§èƒ½

### å¯é€‰ä»»åŠ¡ï¼ˆå±æ€§æµ‹è¯•ï¼‰
- [ ] 6.5 Write property test for score ranges
- [ ] 6.6 Write property test for retry logic
- [ ] 6.7 Write property test for iteration limit

---

**å®Œæˆæ—¥æœŸ**: 2024å¹´12æœˆ27æ—¥  
**å®ç°è€…**: Kiro AI Assistant  
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
